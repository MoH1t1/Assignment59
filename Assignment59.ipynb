{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1. What is Bayes' theorem?\n\nBayes' theorem describes the probability of an event based on prior knowledge of related events. It provides a way to update the probability of a hypothesis as more evidence or information becomes available.\n\n# Q2. What is the formula for Bayes' theorem?\n\nThe formula for Bayes' theorem is: P(A∣B)= P(B∣A)P(A) / P(B)\n\nwhere:\nP(A∣B) is the posterior probability (probability of A given B).\nP(B∣A) is the likelihood (probability of B given A).\nP(A) is the prior probability of A.\nP(B) is the probability of B.\n    \n# Q3. How is Bayes' theorem used in practice\n    \nBayes' theorem is used in:\nMedical diagnosis: To update the probability of a disease based on test results.\nSpam filtering: To classify emails as spam or not based on prior probabilities and features in the email.\nMachine learning: In classification algorithms like Naive Bayes, where the goal is to predict the category of a given data point.\n\n# Q4. What is the relationship between Bayes' theorem and conditional probability?\n\nBayes' theorem is a direct application of conditional probability. It allows us to compute the probability of a cause (hypothesis A) given an effect (evidence B) by using \nthe reverse conditional probability P(B∣A), along with prior knowledge of the probabilities of the cause and effect.\n\n# Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n\nThe type of Naive Bayes classifier to use depends on the nature of the features:\n\nGaussian Naive Bayes: For continuous features that are assumed to follow a Gaussian (normal) distribution.\nMultinomial Naive Bayes: For discrete count features, such as word counts in text classification.\nBernoulli Naive Bayes: For binary features, where each feature is either 0 or 1 (e.g., presence or absence of a feature).",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n# Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n# each feature value for each class:\n\n# Class       X1=1    X1=2     X1=3      X2=1      X2=2    X2=3     X2=4\n# A           3        3         4        4         3       3         3\n# B           2        2         1        2         2       2         3\n\n# Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?\n\n\nTo solve this, we can apply the Naive Bayes classifier. Formula for Naive Bayes for classification is: P(Class| X1,X2) = P(X1,X2,|Class) P(Class)/ P(X1,X2)\n\nSince we are assuming equal prior probabilities for each class, P(Class) will be the same for both classes. This means we can focus on comparing the numerator for each class, \nas the denominator will cancel out in both cases.\n\nFor each class, we need to calculate the likelihood of observing the new instance, given the features X1=3 and X2=4, using the conditional probabilities. Let’s calculate the likelihoods for each class:\n                                                                                                                                                    \nFor class A:\nP(X1 = 3|A) = 4/ 3+3+4  = 0.4\nP(X2 = 4|A) = 3/ 4+3+3+3  = 0.231\nThus, the likelihood of Class A for the new instance is:    P(X1 =3, x2 =4|A) = 0.4 X 0.231 = 0.0924\n                                                                                                                                                    \nFor class B:\nP(X1 = 3|B) = 1/ 2+2+1  = 0.2\nP(X2 = 4|B) = 3/ 2+2+2+3  = 0.333\nThus, the likelihood of Class B for the new instance is:    P(X1 =3, x2 =4|B) = 0.2 X 0.333 = 0.0666\n                                                                                                                                                    \nSince P(X1 = 3, X2 = 4∣A) is greater than P(X1 = 3, X2 = 4∣B), the Naive Bayes classifier will predict Class A for the new instance.\nSo, Class A is the predicted class.                                                                                                                                                ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}